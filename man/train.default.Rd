% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/training.R
\name{train.default}
\alias{train.default}
\alias{train.modData}
\title{Regularized linear model tuning and training.}
\usage{
\method{train}{default}(
  x,
  y,
  trans_fun = identity,
  standardize = FALSE,
  alpha = 1,
  family = "gaussian",
  type.measure = c("default", "mse", "deviance", "class", "auc", "mae", "C"),
  safe = FALSE,
  ...
)

\method{train}{modData}(
  x,
  y,
  trans_fun = identity,
  standardize = FALSE,
  alpha = 1,
  family = "gaussian",
  type.measure = c("default", "mse", "deviance", "class", "auc", "mae", "C"),
  safe = FALSE,
  ...
)
}
\arguments{
\item{x}{a numeric matrix with training explanatory factors, features are
specified in rows, observations are provided as columns. Alternatively, a
\code{modData} class object created e.g. with \code{\link{pre_process}}.}

\item{y}{a numeric or factor matrix with the observed responses
(e.g. phenotype in the training data). The responses are provided as columns,
the observations are given in rows. The row names of \code{y} must match column
name of \code{x}.}

\item{trans_fun}{a numeric function to transform responses for modeling.
A normality-improving transformation such as \code{log} may be provided here.
Used only for Gaussian family models and ignored elsewhere.}

\item{standardize}{logical, should \code{x} values be standardized prior
to modeling?}

\item{alpha}{numeric mixing parameter, which defines the modeling algorithm
(RIDGE regression: \code{alpha = 0}, LASSO: \code{alpha = 1}, Elastic Net otherwise).
See \code{\link[glmnet]{glmnet}} for details.}

\item{family}{family of the \code{glmnet} models. Currently, only 'gaussian',
'binomial', 'poisson', and 'multinomial' are implemented.
See: \code{\link[glmnet]{glmnet}} for details.}

\item{type.measure}{model selection statistic (i.e. loss function). Defaults
to deviance. See \code{\link[glmnet]{cv.glmnet}} for details.}

\item{safe}{logical, should modeling proceed in spite of errors for
some responses? .
If \code{TRUE}, any errors are logged in \code{errors} element of the output. This
option may be, however, considerably slower.}

\item{...}{additional arguments passed to \code{\link[glmnet]{cv.glmnet}} and
\code{\link[glmnet]{glmnet}}.}
}
\value{
An object of class \code{modTrain} with the following components:
\itemize{
\item \code{models}: a list of \code{\link[glmnet]{cv.glmnet}} models used for
tuning of \eqn{\lambda}, named after the columns of the \code{y} matrix.
\item \code{stats}: the optimal \eqn{\lambda} parameter values, cross-validated
selection measures, and metrics of model performance in the
training data set.
\item \code{errors}: a list of error messages for responses for which tuning/training
failed.
}

Note the \code{nobs()}, \code{components()}, \code{summary()}, \code{plot()}, and \code{predict}
methods defined for the class.
}
\description{
Selects the optimal value of the shrinkage parameter \eqn{\lambda} by
minimizing model deviance/error or maximizing model accuracy
in cross-validation.
Subsequently, \code{\link[glmnet]{glmnet}} models are trained
(one for each column of the \code{y} response matrix) and returned along with the
optimal \eqn{\lambda} values, basic cross-validation stats and,
for Gaussian and Poisson family models, with additional metrics of goodness
of fit.
}
\details{
Model selection via cross-validation is performed by
\code{\link[glmnet]{cv.glmnet}}.
Independently of the modeling family, the function returns the tuning
statistic in out-of-fold predictions for the best \eqn{\lambda} values.
Specifically, for Gaussian family, the function calculates also
raw \eqn{R^2} with the formula \eqn{R^2 = 1 - \frac{MSE}{Var(y)}} for
out-of-fold predictions, where MSE stands for the out-of-fold
mean squared error.
For the Gaussian and Poisson family, Pearson's r and Spearman's
\eqn{\rho} for the correlation of predictions with the observed response
in the entire training data set are computed.
Additional evaluations for classification, i.e. binomial and multinomial
models are overall accuracy and Cohen's \eqn{\kappa} computed for comparison
of the observed and predicted class assiggnment in the training data set.

The function will run in parallel provided a backend compatible with
\code{\link[furrr]{future_map}}
(e.g. declared by \code{\link[future]{plan}}).
Observations with missing response entries are silently removed.
Note: \code{glmnet} may have problems with fitting model to messy data
(i.e. a lot of NA) and may also fail if there are negative values in the \code{y}
matrix. If this is the case, you may try transform the responses accordingly.
The \code{train()} function is a S3 generic.
}
\references{
Friedman J, Hastie T, Tibshirani R. Regularization paths for generalized
linear models via coordinate descent.
J Stat Softw (2010) 33:1â€“22. doi:10.18637/jss.v033.i01
}
